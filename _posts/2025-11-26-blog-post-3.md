---
title: "GPUs in AI: Understanding the design of NVIDIA GPUs from the ground up, with AI compute cluster considerations"
date: 2025-11-26
permalink: /posts/2025/11/kernels-and-a-ieee754-representability-proof/
tags:
  - precision
  - mantissa
  - fp32
  - fp64
  - fp8
  - bfloat16
---

## Takeaways
- Designed a kernel, which faces a bug. Understanding and debugging the issue teaches you a lot about the limitations of floating point representations
- Delved deeper into the mathematics of IEEE-754, and common implementations of the standard such as bfloat16, fp32, fp64, and fp8
- Explained why an unsigned 32 bit integer may be more precise in workloads such as hashing than FP32 
- Conclude with a mathematical proof that characterizes the cases in which a 32-bit integer will be more precise than floating point schemes like FP32

Full version on Substack â†’ [**Curb your memory hierarchy**](https://eyeamansh.substack.com/p/a-kernel-bug-and-an-intriguing-proof)

---

## Summary 

Starting out in kernels and facing inexplicable bugs? Yeah, that was once me. Check this blog out for a walk through concepts that typically are seen as obscure in the high level programming world. We finish with a proof with practical debugging implications to weed out integer/floating-point precision bugs. 